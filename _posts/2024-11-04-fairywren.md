---
layout: post
title: "Scanning OSDI'24 | FairyWREN: A Sustainable Cache for Emerging Write-Read-Erase Flash Interfaces"
date: 2024-11-03 21:34:00
description: reading fairywren now
tags: flash-cache, osdi, scanning, paper
categories: sample-posts
images:
  slider: true
---

上一次的分享，我讨论了云存储中的闪存缓存。今天，我将继续围绕闪存缓存的主题，但这次关注的是数据中心中的闪存缓存。我今天要分享的这篇论文名为《FairyWREN》，是由卡内基梅隆大学（CMU）团队和微软公司合作，发表在今年的计算机系统顶会 _OSDI_ 上。这篇论文提出了一种用于新兴写-读-擦除闪存接口的可持续缓存方案。

我的分享将从以下几个方面展开：背景、动机、设计、测试以及总结。

## 背景

近年来，碳排放和碳减排问题越来越受到关注。数据中心的碳排放占据了全球总碳排放中的重要份额。根据论文的数据，预计到2050年，数据中心的碳排放将占全球总碳排放的33%以上。为了应对这一挑战，像亚马逊、谷歌、Meta、微软等大公司都在积极寻求实现碳的净零排放。根据《2030数据中心白皮书》，零碳节能已经成为未来数据中心的一个重要目标。

借用作者在OSDI演讲中使用的一张图，我们可以看到，数据中心的碳排放主要分为两类：运营碳排放和隐含碳排放。运营碳排放相对容易控制，比如通过使用太阳能、风能等可再生能源。然而，隐含碳排放则相对较难，逐渐占据了主导地位，达到了80%以上。隐含碳排放主要来源于数据中心基础设施的生产、运输、功耗和最终报废，根本原因在于硬件设备的一次生命周期。解决隐含碳排放问题的关键在于延长硬件设备的寿命，或者使用更低功耗的硬件。

了解了数据中心的碳减排后，我们再来看数据中心的存储体系。存储是数据中心的核心任务之一。现在主流的存储器件主要是几类：HDD，俗称的机械硬盘。SSD，俗称的固态硬盘，因其基于闪存介质，又叫做闪存存储。而DRAM就是常说的内存。不同层级的存储器件，其速度和成本不同。通常来说，如表中所示，内存DRAM的成本大于固态硬盘SSD，大于机械硬盘HDD，均差在一个数量级。在大规模的数据中心中，缓存不会全部使用DRAM，存储也不会全部使用SSD，而是通常采用一种折衷的存储方案。中间的结构被称为“闪存缓存”，它连接了主存和二级存储。

## 动机

在动机上。使用闪存缓存有一个很好的优势：碳减排。内存和存储通常占据服务器碳排放的46%和40%。相比用DRAM做缓存，闪存缓存具备更低的功耗，能够减少高达12x的隐含碳排放。此外，近年来，闪存技术也在朝着更高密度的发展。一个存储单元从原来只能存储1比特，也就是SLC，发展到现在能够存储4比特，也就是QLC。相同硅片上能够存放更多比特，进一步降低生产成本，减少隐含碳排放。

目前，数据中心通过延长服务器的使用寿命来减少隐含碳排放。这相对容易达到，微软和meta等公司已经实施。但与之相比，闪存设备则相对困难，其具备非常有限的写入寿命。随着闪存密度的增加，其使用寿命进一步下降。要想达到6年写入寿命，只能限制每日的写入量非常小。从右图可以看到，纵向看不同颜色的线，使用寿命越长带来的碳排放和成本的模型值越低。从横向看不同的介质，随着存储介质密度的增加，闪存设备每日写入量的限制更加严格。

然而，与每日写入量的严格限制相违背的是，假设不加以精心设计，闪存设备上会存在特别严重的写入放大。这是闪存设备的常见问题，体现为写有效数据时，会重复写入很多额外的数据。分析其主要原因包括两种：第一种是应用级写入放大。在闪存中，数据通常按写入单元（例如64KB）进行存储。如果只想写入少量数据（例如100字节），那么就会先读取64KB的页面，修改其中的100字节，然后重新写入64KB的页面，导致最多640倍的写入放大。第二种是设备级写入放大。当闪存存储空间满时，设备会进行垃圾回收，将有效数据聚集到一起，然后擦除原来的擦除单元。由于擦除单元的大小通常是GB级别，而写入单元只有64KB，导致有效数据的碎片，要频繁搬迁。搬迁相同的数据造成了设备级的写入放大。可见如果没有合理的设计，闪存设备将面临严重的写入放大问题，这也是本篇论文所聚焦的核心问题之一。


基于前面的分析，作者提出了设计可持续、低碳排放闪存缓存的三个核心要点：首先，要减少无用的闲置空间，因为闲置空间会直接提高隐含碳成本；其次，需要尽可能减少DRAM的使用，从而降低能耗；最后，要尽量降低写入放大，以减缓设备的磨损速度。与传统的DRAM缓存和键值存储相比，闪存缓存的设计思路更为复杂。DRAM缓存无需考虑寿命问题，而键值存储则缺乏快速删除的能力，这些都使得它们无法直接应用于闪存缓存的设计。

在优化闪存缓存方面，已有多项重要研究成果，且均发表在计算机系统领域的顶级会议上，如OSDI和SOSP。  第一个典型的方案是基于日志结构的闪存缓存。这种方法的核心原理是对缓存对象进行顺序的追加写，同时为每个对象建立一个对应的DRAM索引。顺序写的好处在于能够解决写入放大的问题。然而，当缓存的对象非常小时，DRAM索引的开销会变得极为庞大。例如，要缓存2TB的100字节对象，所需的DRAM索引高达75GB。

第二个是，发表在OSDI'20上的CacheLib。CacheLib采用了组相联缓存的设计，利用哈希函数替换掉了原来的DRAM索引，将每个对象映射到指定的唯一集合内，当集合写满了以后就进行缓存的替换。这样的好处是极大的降低了DRAM索引，但缺点在于带来了小对象的随机写，每次写入一个100B的小对象，都会带来几十倍到几百倍的写入放大。

第三个是，发表在SOSP'21年的最佳论文Kangaroo。Kangaroo结合了日志结构和组相联缓存的优点，提出了一种分层架构设计。具体而言，它在写入小对象时不急于立即进行哈希操作，而是先通过顺序写实现日志化缓存，随后再以集合为单位执行哈希操作。这种设计在减少应用级写入放大和DRAM索引方面取得了显著效果。然而，Kangaroo的层次设计虽然缓解了小数据与4KB写入单元的失配问题，但4KB写入单元与GB级别擦除单元的失配依然存在。这种失配仍然会导致设备级写入放大的问题。

设备级写入放大的根本原因在于传统的LBAD接口效率的低下。LBAD接口最初设计是为了简化从机械硬盘到固态硬盘的过渡，它屏蔽了设备的物理层细节，只暴露逻辑层。像Kangaroo这样的设计，其所有缓存操作，包括缓存的驱逐和替换，都基于逻辑层完成，而设备的垃圾回收则是完全自主的。这种逻辑与物理层的隔离带来了一个潜在问题。

可以考虑一种较坏的情况：例如，设备刚刚完成一个有效数据块的搬迁，但随后该数据却因缓存替换而失效，从而导致一次无意义的写入。对于密集覆写的缓存场景，这一问题尤为突出。

近年来，新兴的WREN接口（如ZNS、FDP等设备）为解决这一问题提供了新的机遇。WREN接口允许上层控制所有的写入操作，包括垃圾回收。如右图所示，这种新接口能够显著提升写入的可控性。作者将这种接口进行了规范化定义：第一，包含三种WREN操作，即写入、读取和擦除；第二，同时要求擦除操作必须以整块为单位，并由上层完全控制；此外，设备还需支持多个活跃的擦除单元。基于这一规范化定义，作者在软件层面设计了对应的闪存缓存架构。


通过硬件和软件的协同优化，FairyWREN成功实现了可持续闪存缓存的所有设计目标。


## 实现


在设计上，FairyWREN采用了一种分层的架构。具体而言，它由两个主要模块组成：LOC模块和SOC模块。第一个是LOC模块，用于缓存大于2KB的大对象，初步实现大小对象的分离。LOC通过日志结构实现顺序写入，并在DRAM中建立对象索引。由于对象较大，这个内存开销是可控的。这里的一个关键设计是，为了适配新接口的特性，FairyWREN将缓存段的大小对齐为一个可控的擦除单元。它的缓存插入和缓存查询都较为直接。为了设备级写入放大，本篇工作额外引入了缓存替换逻辑，后面一起介绍。
第二个是SOC模块，用于缓存小于2KB的小对象。小对象不能够采用原先的设计，因为内存开销会很大。SOC做了分层设计：包括DRAM、少量日志结构缓存、大量的组相联缓存。缓存插入会优先插入到高层，再依次驱逐到低层。这里的一个关键设计是，同样为了适配新接口的特性，FairyWREN实现组相连缓存的变体，将Set的随机写变成了顺序追加，同时为每个Set建立内存索引。但这也带来了更多的内存索引开销，作者后续对此进行了相应设计。作者同样对SOC额外引入了缓存替换逻辑。

作者最核心的贡献在于，将传统闪存缓存的替换逻辑、设备的垃圾回收逻辑，原本分割的两者结合在一起，避免了设备层级进行无效的垃圾回收。首先，对于LOC模块，由于其是单层、顺序追加写以及对齐的设计。缓存替换只发生垃圾回收时，只需要通过LRU等方式擦除整个擦除单元即可。而SOC模块，是双层、非对齐的设计，这是论文设计的难点，也是关键点。

SOC采用了嵌套打包的算法，对于双层结构，各自空间满了之后，都会进行垃圾回收。FairyWREN从中选出一个擦除单元进行回收。此处的垃圾回收，并不是传统设备上无效的迁移。相反，闪存缓存会对回收的单元进行分析，选出自己认为有用的Set进行迁移。具体是如何选取有用的Set的？FairyWREN会将EU进行散列，恢复出Set结构。然后会检索上层结构得出哪些Set是需要回收，哪些Set需要进行重组。迁移完上述有用的Set后，FairyWREN擦除原来的单元。总结是，闪存缓存也可以控制垃圾回收，将其缓存替换相结合，避免了无效重复写。

其他设计上，时间限制，我做简单介绍。进行前面的嵌套打包时，有可能只回收一个对象，还是会带来整个Set的写入。我们把经常访问或经常写的特性，叫做热。对此FairyWREN将Set进行了冷热划分，同时将对象也进行了冷热划分。FairyWREN识别出热对象后，将其放到冷SubSet中。这样一来，只需要一直更新热SubSet即可。当然，冷热子集的对象的冷热程度会随插入而发生改变，所以需要定期进行冷热重组。

第三个设计，前面的设计带来了Set的内存索引，而Set的冷热划分，进一步增大了这个索引。如何控制好索引开销，是一个关键的问题。FairyWREN通过切片、共享和双缓冲的方式缓解这个问题。切片散列的方式可以降低索引位数。然而，不可能为每个切片分配一个擦除单元。FairyWREN引入了共享擦除单元的设计。但共享会导致不同切片写满的速度不一样，这会导致写会闪存时的碎片化的问题。FairyWREN进一步引入了双缓冲区的方式，让快的写满后等一下慢的，缓解了这一问题。最终DRAM开销相比SOTA控制在19%以内。

## 测试

在测试上，测试环境配置中，作者基于CacheLib、使用模拟和真实的ZNS设备，基于Meta和推特的Trace进行，并对SSD寿命和碳排放进行了建模。对比对象包括：理想的写入、日志结构缓存、SOTA Kangaroo和Kangaroo简单移植到ZNS的版本，也就是不做额外设计。

从总体的碳排放结果中，可以看到FairyWREN对比SOTA Kangaroo总体排放量减少21.2%，最核心原因在于FairyWREN的闪存碳排放最少。

在磨损速度和性能的测试上，FairyWREN对比SOTA Kangaroo减少了12.5x的写入放大，也就是磨损速度。性能均比Kangaroo要好。并且对于缓存而言，这些优势并没有以牺牲缓存命中率为代价。

作者进一步研究了不同命中率下的影响，对比了直接将Kangaroo搬到新接口的方案，发现FairyWREN接近理想情况，且直接将Kangaroo搬到新接口的优化效果不大。

作者在各种高密度闪存上进行测试分析，认为只有FairyWREN能够运用到高密度闪存QLC和PLC上。具体是哪些技术点带来了这些优势？作者进一步进行分解测试，可以看到嵌套打包和冷热分离两个技术均贡献了多倍的优化。

## 总结

以上是论文的全部内容。下面我对这篇论文进行简单小结。这篇论文主要背景是数据中心的碳排放问题，存储器件的碳排放贡献很大。然而问题是，高密度闪存带来低碳的契机，但其寿命受到严峻挑战。传统的LBAD接口会带来两级写入放大加剧寿命磨损。现有的缓存方案，并不符合可持续使用的要求。机遇是近年来出现了新的WREN接口，围绕其新硬件特性作者对闪存缓存进行了软件设计，最终解决上述的问题。总的来说，在软件设计上，作者的方案是：提出适配到新接口的设计架构、通过嵌套打包算法统一垃圾回收和缓存写入，进一步作者还实现了冷热对象分离和DRAM开销优化，使得系统整体上表现最佳。

本文亮眼之处在于其总结性的写作方法、让人眼前一亮的新背景、以及扎实的建模和实验。作者团体在本次OSDI投稿前，就已经在HotCarbon'24上发表过有关存储器件碳排放建模分析的文章，功底很扎实。论其不足之处，我认为是过于简单的设计，对工作独到之处没有明确，需要读者对比阅读和总结。


参考我的论文解读PPT：<a href="https://darongyang.github.io/blog/assets/pdf/FairyWREN-v1.pdf"> FairyWREN-Reading-PPT </a>

发现CMU的flash cache三部曲（cachelib->kangaroo->fairywren）都发在osdi/sosp上，工作的连续性还是让我深感震惊和佩服 :-)


## QA

Q1：从报告中看出，设备级的写入放大，其主要原因是写入单元和擦除单元的失配问题。我想请问，设备级写入放大能否通过减小擦除单元来进行减小或擦除？
A1：一般说来，是软件围绕硬件开展设计。硬件处于这种设计的原因，更多还是出于成本的考虑。作者也有对这个做了一点分析，文章中说到要将擦除单元从GB降低到KB量级才会有效果，然而这在现在硬件实现上几乎是不可行的。所以不能够想当然的这样弥补。

Q2：FairyWREN从碳减排的角度出发，我想问这个工作减少了闪存缓存上的碳排放，有没有增加其他方面的消耗呢，例如CPU上。
A2：作者的核心贡献，是将垃圾回收和缓存驱逐相结合，并没有带来额外上计算上的碳排放开销。确实对于DRAM上会有部分开销，但作者进行了优化，控制了开销程度。最后通过作者的建模，闪存减少的开销弥补了DRAM的开销，并且取得更好效果。


Q3：我想问有关冷热对象分离的问题，第一个是作者是如何识别冷热对象的，以及识别准确度有多少？为什么这个区间多分几份，优化效果就更好？
A3：冷热对象的识别方式不是作者设计的核心，是直接利用的现有的一些算法RRIP，这些常用于处理器判断的算法。针对这个算法准确性没有做太多评估。但作者提到区间的过度划分会导致RRIP算法带来缓存不命中率的增加，因此不宜过多划分。

Q4：正如背景所提到的，闪存缓存是DRAM和HDD的连接器，为什么整篇文章的介绍里，看到了很多和DRAM联合设计的介绍，但没有见到闪存缓存和HDD之间联合设计的内容？
A4：本质是数据的传输在金字塔中是从上到下的单向传输。作者考虑DRAM设计，是因为数据会先经过DRAM，才到闪存缓存。作者的目光只聚焦在闪存缓存，讨论的最深内容仅限于闪存缓存的驱逐操作，因为驱逐下去就和闪存缓存无关了。具体从闪存缓存驱逐出去的数据如何进一步在HDD管理，是下层存储HDD该考虑的事情，不是本文关注的范畴领域。

<hr>

以下部分的内容，需要重新整理与合并，暂时没啥时间


## 2 研究动机

### 2.4 关键动机：LBAD接口的DLWA（GC的写放大）

Kangaroo基于传统的LBAD接口。LBAD接口屏蔽了物理层，只暴露逻辑层。Kangaroo的缓存驱逐/替换操作基于逻辑层，LBAD的垃圾回收操作基于物理层，两者互不知情。那么就会带来：GC经常会重写一些无效的数据。表现为：GC刚搬迁完，这个数据就被替换而无效了。对于密集覆写的闪存缓存来说，这个问题尤为显著。因此，作者认为垃圾回收是缓存替换的一个机会。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/LBAD-GC.png" title="architecture" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    LBAD接口会执行上层透明的垃圾回收
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/LBAD-rewrite.png" title="architecture" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    透明垃圾回收的块随后会被重写
</div>



## 3 设计实现

（想把最头大的这部分先捋清楚...）

**Key insight**：将垃圾回收和缓存准入进行统一，无论在线写入还是垃圾回收都能够进行对象准入。（换句话说，就是借助WREN让垃圾回收的时候做到object attention，主动判断哪些对象要重写/回收，避免像LBAD那样无效回收）

### 3.1 系统结构

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/architecture.png" title="architecture" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    FairyWren系统结构
</div>

- LOC：缓存大对象（>2KB）
	- DRAM：EU大小的segment buffer + 大对象索引
	- LOC：采用日志化结构缓存，执行大的顺序写。适应到WREN很简单，segment大小定为一个EU即可。由于对象较大，其DRAM开销较小。
	- 两种缓存操作：（1）**插入**。先插入DRAM并建立索引，segment满了一起写闪存。当插入发生替换时，采用GC相结合的设计，见3.2。（2）**查询**。查询查找object的key对应的地址，然后从闪存读取。
- SOC：缓存小对象
	- DRAM：FwLog的segment buffer + FwLog的小对象索引 + FwSet的set索引
	- FwLog：采用日志化结构缓存，缓冲小object以便可以高效写入FwSet。小对象索引DRAM开销大，为了保证其开销，FwLog只占SOC的5%。
	- FwSet：采用组相连缓存。对小对象进行日志化缓存会DRAM爆炸。WREN接口不支持随机写，FwSet适配到WREN中要遵循日志结构存储，将set（大于4KB）作为日志化追加的对象（关键设计）。set索引开销小。
	- 两种缓存操作：（1）**插入**。采用类似于LOC方式先插入到FwLog，如果FwLog满了，会插入到FwSet中。FwSet的插入可能会进一步导致FwSet的替换，采用GC相结合的设计，见3.2。（2）**查询**。首先采用类似于LOC方式在FwLog查询，如果找不到再hash得到对象在FwSet中对应的Set。在Set内顺序扫描找到对象。

### 3.2 垃圾回收和在线写的统一

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/nest-pack.png" title="architecture" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    nest打包方法：SOC的垃圾回收和驱逐流程
</div>

- LOC
	- 当LOC满的时候，FairyWREN进行替换。驱逐时，由于segment和EU对齐，通过LRU或FIFO移除整个segment，即EU即可，完成WA=1的擦除。
- SOC（关键设计）
	- **朴素方法**。当FwSet或FwLog满的时候，需要进行垃圾回收。FairyWREN同样将其和缓存替换相结合。一个最朴素的方法是直接将末尾的EU驱逐，但冷热对象混杂会大大降低命中率。
	- **FairyWREN的“嵌套打包”**。（1）**EU选择**。FairyWREN从FwSet和FwLog中选择一个EU进行垃圾回收。（2）**Set散列**。FairyWREN会将选中的EU的所有对象哈希散列为若干Set（如果EU在FwSet中，本身就已经是若干的Set）。（3）**Set重组**。对于这些Set，FairyWREN会检索出FwLog中位于Set的所有对象。将这些Set的对象重组为一个新的Set，其中可能会逐出不必要的对象。（4）**重写与擦除**。然后将重组的Set，重新追加写入到FwSet，并擦除上述EU。
	- **如何work**？设计的最关键在于：合并了之前两个不同的过程，实现垃圾回收和缓存写入与驱逐/替换的协作。最坏的例子（如前面2.4所介绍）：LBAD的垃圾回收刚搬迁完一个Set，然后FwLog来了一个新对象后，又要马上被重写。在LBAD中，无法实现写入合并。而FairyWREN基于上述的嵌入打包，消除了很多不必要的写入。

### 3.3 KwSet的冷热对象分离

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/hot-cold-split.png" title="hot-cold-split" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    冷热对象分离
</div>

- **动机：** 每次从FwLog插入一个对象到FwSet，都会带来整个Set的重写。
- **冷热对象分离**。（1）**Set划分**。将KwSet划分为两个SubSet，频繁更新的热SubSet和不频繁更新的冷SubSet。冷热SubSet分别处于不同的EU中。（2）**对象识别与放置**。冷热对象的识别通过RRIP算法完成。值得注意，对于闪存缓存来说，热对象要放到冷SubSet中，因为其不会频繁被“驱逐”，会常驻于KwSet中；冷对象与之相反。这样一来，每次FwLog到FwSet的散列插入对象，只需要对KwSet中的热SubSet频繁更新即可。（3）**冷热重分类**。 但不能一直插入热SubSet，因为新插入的对象可能是热对象。因此Set每进行n次（设定的阈值）嵌套打包操作后，会进行一次全SubSet的读取（包括冷和热SubSet）。Set中的对象会重新进行合并和分类，将上述热SubSet中新插入的热对象移动到冷SubSet中。
- **讨论**。这个方法能带来接近一半的写放大优化。eg: n=5, Set=8KB，能实现40KB->24KB的写入量减少40%。但，也不是划分越多越好。而WREN只支持有限的EU（10个）。FairyWREN使用4个EU。此外，冷热分离会是的cache命中率下降，识别未必是准确的。


### 3.4 DRAM使用优化

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/dram-reduce.png" title="hot-cold-split" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    双缓冲区解决碎片化问题
</div>


- **设计：日志结构切片**。FwLog的object，FwSet的Set都是按照日志结构存储，分别需要object和Set的索引，DRAM开销很大。对日志结构存储空间进行 $2^n$ 切片可以减少n比特索引。（FwLog划分64份，FwSet划分8份）
- **朴素方法**。一种朴素的思路是一个分片用一个EU/segment，然而EU数有限。不切实际。
- **设计：共享EU/segment和碎片化问题**。所有切片共用一个sgement/EU，将DRAM中的segment分成 $2^n$ 份切片区域。各个切片将对应的区域写满后，再一次全部写到EU中。然而这会导致：部分区域可能提前写满，而其他区域没写满时就得写回EU，导致碎片化问题。
- **设计：双缓冲区**。FairyWREN采用了双缓冲区来延缓这个问题，写的快的区域，可以写入到副本缓冲区中的新区域，起到一定的“快”等”慢“的作用。等主缓冲区尽可能写满后，就写回到EU中。（效果很好，将碎片化问题限制在了1%）
- **设计：更大的Set**。此外，FairyWREN还通过增大 Set大小来减小 FwSet 索引。（作者讨论了更大的Set不会带来写放大可控，即5%）

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/fairywren/dram-result.png" title="hot-cold-split" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    DRAM开销定量对比
</div>

最后，FairyWREN 使用的 DRAM 仅比 Kangaroo 多 19%，但后续会看到比 Kangaroo带来了12.5x写入量的减少。


## 个人评价

- 技术写作还是很难懂的，如果之前没读过Kangaroo和CacheLib。很多细节略过，不知其所以然。
- 创新点真的太weak了，写来写去全是和kangaroo一个模子刻出来的。至少目前看起来，大小object分离、Fwlog和Fwset分层、冷热分离等等都是kangaroo提到过的。晦涩难懂又没创新。
- design开始看的真的折磨

## reference

- <a href="https://www.usenix.org/conference/osdi24/presentation/mcallister"> FairyWREN | OSDI'24 </a>
- <a href="https://saramcallister.github.io/files/2024-osdi-mcallister-slides.pdf"> FairyWREN Presentation Slides</a>
- <a href="https://mp.weixin.qq.com/s/0g1jBn9SdE4QwygKx2qwQQ">【论文解读】数据中心节能减排：可持续的闪存缓存设计 FairyWREN (OSDI'24) </a>
- <a href="https://www.zhihu.com/question/649626302/answer/3596509565"> 2024年操作系统设计与实现研讨会（OSDI）有哪些值得关注的文章？--FairyWREN </a>
- <a href="https://zhuanlan.zhihu.com/p/708037149"> OSDI 2024 论文评述 Day 3 Session 9: Data Management | IPADS-SYS </a>